- [Serverless 如何应对 K8s 在离线场景下的资源供给诉求](https://www.cnblogs.com/tencent-cloud-native/p/14275579.html)



**K8s 架构下的应用混部**，则是一个较特殊的领域，一方面大部分的企业基础设施升级为云原生架构后，通常会天然支持一些混部能力，从而带来一些显而易见的收益，比如资源利用率的提升。可以说容器化和 K8s 为整个行业进入应用的大规模混部打开了一扇窗。而另一方面，但当我们真正进这个领域时，即使站在 K8s  的肩膀上，混部依然是对企业架构能力的一个巨大挑战。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094037454-1907496633.png)

在容器化之前，在物理或虚拟服务器上部署应用，资源利用率通常很低，一是很多应用本身具有潮汐现象，二是服务器大部分情况只能部署一个应用，而非 K8s 那样在一个节点上部署多个。但容器化托管到 K8s 集群后，很多时候，我们会发现资源利用率还是不高。

上图，是一个 K8s 集群线上业务的典型资源曲线，最上面的蓝线是容器资源 request 申请值，红色线是容器真实运行的曲线值，我们看到  request 和 usage 之间有很大 gap，这是因为对容器资源的评估不可能完全精准，另外，波峰和波谷也有差别，最终导致平均利用率不高。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094038260-1359891045.png)

那是不是 K8s 不行呢？当然不是，K8s 在助力我们进行应用混部上虽然还没有解决所有的问题，但绝对是最佳的可选平台之一。

优秀的系统能力使 K8s 天然适合进行混部，包括在线服务的混部和现在业内火热的在离线混部。腾讯内部也通过 K8s 化，在很多场景显著提升了资源利用率。

像腾讯这种规模的计算体量，除了大家熟知明星应用，还有海量的算力在进行服务支撑、离线计算等。通过把这部分应用以及一些潮汐现象明显的产品服务进行混部，资源利用率的提升非常显著。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094039476-1634256475.png)

在业内，Google 基于 K8s 的前身 Borg 系统，从 2015  年至今已发布了多篇与混部相关的论文。于去年发布一篇论文中，可以看到 Borg 支持的混部能力已经逼近 60% 的 CPU 资源利用率。对比其  2011 年和 2019  年的混部效果，可以看到，除了利用率的提升，最显著的变化，一是应用分级粒度更细了，二是各级别的应用运行更加平稳了。尤其是第二点，明显感觉到  Borg 在混部的支撑层面，如调度增强、资源预测回收、任务避让等机制上的进步。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094040649-1328503535.png)

提升混部效果的关键是什么？首先，我们需要明确两个问题。

**第一个问题，混部的目的是什么？**混部的目的是在保证在线业务服务质量的前提下，实现闲置资源复用，提高整体资源利用率。保证在线业务服务质量是前提，使之不受影响，没有这个前提，利用率提升再高也毫无意义。

**第二个问题，什么样的应用适合混部？**适合混部的应用有两类：一类是算力要求很高的周期性应用，通常是一些离线计算任务。一类是容易造成资源浪费的应用，通常是一些长时间运行的、具备潮汐现象的在线服务。但要注意，有些在线服务会对某些资源有较高的敏感性，这类服务是对混部系统的最大挑战，因为稍有不慎就会偏离混部的目的，影响了在线服务质量，得不偿失。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094041417-209895940.png)

在确定了这两个问题之后，我们来看下混部系统需要有哪些机制。通常分为三层：

一是对混部应用进行特征画像、定级以及分配资源配额的应用管理层。这一层定义应用的级别，混部的时机，以及通过配额保证资源分配不失控。

二是对混部集群进行调度、隔离、资源复用和避让的核心系统。这一层是混部的核心，基本决定了我们的集群能达到什么样的混部效果。

最后，还需要一整套适配的自动化运营系统。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094042754-468240774.png)

混部的基本原理是对闲置资源的再分配。通常，闲置资源有两个来源。集群内会有碎片资源，这是资源分配颗粒度问题导致的，这些碎片资源可以分配给颗粒度更小的应用使用。另外，在线服务申请的资源通常会大于实际使用的资源，配合应用画像，预测出这部分服务的波峰波谷，可以将这部分闲置资源分配给其他应用。

基于这个背景，引申出混部最核心的两个子模块：**资源复用和任务避让**。顾名思义，资源复用就是把上述两种闲置资源通过预测、回收的机制，实时再分配给混部应用使用。而任务避让，就是检测核心在线服务是否收到了混部的影响。一旦发生干扰，马上触发冲突处理机制，进行压制和再调度等操作。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094044228-187725154.png)

可以这么说，这两个模块决定了混部的效果和可混部的应用范围。除了理论上的问题，还有一些重要的点必须考虑：为了保证混部效果，频繁对集群实时情况进行预测和资源回收，对集群本身带来了额外的负担，如何在尽可能资源复用和尽量降低资源预测回收频率之间找到平衡？还有，为了保证在线服务的质量，任务回避通常不可避免，这就降低了次优先级应用的执行效率，高负载时可能导致任务的频繁重试和堆积，进而可能拖累整个集群。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094045319-925938492.png)

为了解决这些问题，腾讯云云原生团队做了一直在思考和尝试，目前较先进的一种方式是通过 serverless 容器即弹性容器，来拓展整个混部集群的资源池。

弹性容器是腾讯云推出的无服务器容器产品。它支持一种能力，类似开源virtual kubelet  的方式，但又相比开源方案能力更强、更适合生产。它支持在一个既有 K8s 集群中通过部署虚拟节点的方式把 pod 调度为弹性容器。调度为弹性容器的 pod 与原集群中的其他 pod 网络互通，如果关联了service ，service间也可互通。

所以无论是已有 workload 的扩容、还是新的 workload，都可以以一种平滑的方式进行调度。且该能力对集群不会产生额外的维护成本。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094046342-1819584610.png)

这个能力对混部的核心价值在于：它无成本的扩展了集群资源池，降低了资源冲突的风险，提升了混部集群的冗余度和适用性。另外，在检测到资源不足之类的冲突时，在很多场景可以不中止次优先级任务，而是视情况扩容或再调度，在弹性容器上继续运行任务，秉持尽量不打断已启动任务的原则，提升整个系统的效率。

![img](https://img2020.cnblogs.com/other/2041406/202101/2041406-20210114094047752-1470325642.png)

**这类混部集群的几个典型场景：**

1、低负载时进行任务填充，运行更多任务，提升集群资源利用率。

2、万一发生了在线服务干扰，封锁相关节点，驱逐次优先级任务到虚拟节点，让其继续运行。

3、发生集群资源紧张时，封锁相关节点，视情况，如果是可压缩资源紧张，比如 CPU、IO  等，则压制次优先级任务；如果是不可压缩资源紧张，如内存、存储等，则驱逐次优先级任务到虚拟节点；在此情况下所有新增 Pod  均调度到虚拟节点，不再对集群固定资源增加任何压力，避免发生雪崩。

