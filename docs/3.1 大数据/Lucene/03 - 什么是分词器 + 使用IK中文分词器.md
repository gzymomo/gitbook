[03 - 什么是分词器 + 使用IK中文分词器](https://www.cnblogs.com/shoufeng/p/9382442.html)



# 1  分词器概述

## 1.1  分词器简介

在对文档(Document)中的内容进行索引前, 需要对域(Field)中的内容使用分析对象(分词器)进行分词.

**分词的目的是为了索引, 索引的目的是为了搜索. **

> - 分词的过程是 **先分词, 再过滤**:
> - 分词: 将Document中Field域的值切分成一个一个的单词. 具体的切分方法(算法)由具体使用的分词器内部实现.
> - 过滤: 去除标点符号,去除停用词(的、是、is、the、a等), 词的大写转为小写.

- 分词流程图:
   ![分词器流程图](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162354187-296980561.jpg)

- **停用词说明**:
   停用词是指为了节省存储空间和提高搜索效率, 搜索引擎在索引内容或处理搜索请求时会自动忽略的字词, 这些字或词被称为"stop words".  如语气助词、副词、介词、连接词等, 通常自身没有明确的含义, 只有放在一个上下文语句中才有意义(如:的、在、啊, is、a等).
   例如:
   ​	原始文档内容: `Lucene is a Java full-text search engine`
   ​	分析以后的词: `lucene java full text search engine`

## 1.2  分词器的使用

(1)  索引流程使用

流程: 把原始数据转换成文档对象(Document), 再使用分词器将文档域(Field)的内容切分成一个一个的词语.

目的: 方便后续建立索引.

(2) 检索流程使用

流程: 根据用户输入的查询关键词, 使用分词器将关键词进行分词以后, 建立查询对象(Query), 再执行搜索.

注意: 索引流程和检索流程使用的分词器, 必须统一.

## 1.3  中文分词器

### 1.3.1  中文分词器简介

英文本身是以单词为单位, 单词与单词之间, 句子之间通常是空格、逗号、句号分隔. 因而对于英文, 可以简单的以空格来判断某个字符串是否是一个词, 比如: `I love China`, love和China很容易被程序处理.

但是中文是以字为单位的, 字与字再组成词, 词再组成句子. 中文: 我爱中国, 电脑不知道“爱中”是一个词, 还是“中国”是一个词？所以我们需要一定的规则来告诉电脑应该怎么切分, 这就是中文分词器所要解决的问题.

常见的有一元切分法“我爱中国”: 我、爱、中、国. 二元切分法“我爱中国”: 我爱、爱中、中国.

### 1.3.2  Lucene提供的中文分词器

- `StandardAnalyzer`分词器: 单字分词器: 一个字切分成一个词, 一元切分法.
- `CJKAnalyzer`分词器: 二元切分法: 把相邻的两个字, 作为一个词.
- `SmartChineseAnalyzer`分词器: 通常一元切分法, 二元切分法都不能满足我们的业务需求. 而SmartChineseAnalyzer对中文支持较好, 但是扩展性差, 针对扩展词库、停用词均不好处理.

**说明: Lucene提供的中文分词器, 只做了解, 企业项目中不推荐使用. **

### 1.3.3  第三方中文分词器

- `paoding`: 庖丁解牛分词器, 可在https://code.google.com/p/paoding/下载. 没有持续更新, 只支持到lucene3.0, 项目中不予以考虑使用.
- `mmseg4j`: 最新版已从https://code.google.com/p/mmseg4j/移至https://github.com/chenlb/mmseg4j-solr. 支持Lucene4.10, 且在github中有持续更新, 使用的是mmseg算法.
- `IK-analyzer`: 最新版在https://code.google.com/p/ik-analyzer/上, 支持Lucene 4.10. 从2006年12月推出1.0版开始,  IKAnalyzer已经推出了4个大版本. **最初是以开源项目Luence为应用主体的**, 结合词典分词和文法分析算法的中文分词组件. 从3.0版本开始, IK发展为面向Java的公用分词组件, 独立于Lucene项目, 同时提供了对Lucene的默认优化实现. **适合在项目中应用. **

# 2  IK分词器的使用

说明: 由于IK分词器是对Lucene分词器的扩展实现, 使用IK分词器与使用Lucene分词器是一样的.
 ![IK分词器](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162432492-709952444.jpg)

## 2.1  配置pom.xml文件, 加入IK分词器的依赖

```xml
<project>
    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <!-- mysql版本 -->
        <mysql.version>5.1.44</mysql.version>
        <!-- lucene版本 -->
        <lucene.version>4.10.4</lucene.version>
        <!-- ik分词器版本 -->
        <ik.version>2012_u6</ik.version>
    </properties>
    
    <dependencies>
        <!-- ik分词器 -->
        <dependency>
            <groupId>com.janeluo</groupId>
            <artifactId>ikanalyzer</artifactId>
            <version>${ik.version}</version>
        </dependency>
    </dependencies>
</project> 
```

## 2.2  修改索引流程的分词器

![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162517598-1253587686.jpg)

## 2.3  修改检索流程的分词器

![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162537169-105351877.jpg)

## 2.4  重新创建索引

使用Lucene默认的标准分词器(一元分词器):
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162556582-456475904.jpg)

使用ik分词器之后(对中文分词支持较好):
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162636270-266092144.jpg)

# 3  扩展中文词库

说明: 企业开发中, 随着业务的发展, 会产生一些新的词语不需要分词, 而需要作为整体匹配, 如: 尬聊, 戏精, 蓝瘦香菇; 也可能有一些词语会过时, 需要停用.

-- 通过配置文件来实现.

## 3.1  加入IK分词器的配置文件

![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162815498-486083471.jpg)

说明: 这些配置文件需要放到类的根路径下.

## 3.2  增加扩展词演示(扩展: 人民邮电出版社)

说明: 在ext.dic文件中增加"人民邮电出版社":
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162832807-1591223420.jpg)

注意: 不要使用Windows自带的记事本或Word, 因为这些程序会在文件中加入一些标记符号(bom, byte order market), 导致配置文件不能被识别.

增加扩展词之后:
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162856567-2137906832.jpg)

## 3.3  增加停用词演示(增加: 的、和)

在stopword.dic文件增加停用词(的、和):
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162914916-1563731497.jpg)

增加停用词之前:
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162934255-727758584.jpg)

增加停用词之后:
 ![img](https://images2018.cnblogs.com/blog/1438655/201807/1438655-20180728162947941-687958096.jpg)

> 注意事项:
>  修改扩展词配置文件ext.dic和停用词配置文件stopword.dic, 不能使用Windows自带的记事本程序修改, 否则修改以后不生效: 记事本程序会增加一些bom符号.
>  推荐使用Emacs, Vim, Sublime等编辑器修改.