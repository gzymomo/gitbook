# 深度学习在时空大数据分析中的应用

来源：钛媒体 	 链接：http://www.tmtpost.com/2579314.html

## 什么是时空大数据？

相信大家已经被深度学习刷屏了，它已经在比如图象识别、语音识别、围棋、扑克……很多领域取得了巨大的成功，引起了多个领域革命性的变化。我今天要讲的跟以上所列都不同，将把重点放在时空大数据分析。

什么是时空大数据呢？大家请看下面这张图。时空大数据有很多的种类，比如说GPS的定位数据，比如说网约车 ——滴滴、Uber的订单的数据，社会网络上的数据，更宏观的国民经济的数据，比如人口密度、人口迁徙的数据。这些数据的共同特点是**它既有时间的属性也有空间的属性**。

![img](https://images.tmtpost.com/uploads/watermark/1400/9421341d7f9faa7df147ed34d6c35f826c7bc4ef_1400_757.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

我们可以考虑有多个时间序列，其中每个时间序列是由一个不同的空间产生的。每个时间序列自身有时间上的依赖关联，而这些时间序列之间也有空间的一些关联。时空大数据的空间上的关联，跟图像上的空间关联有些相似，但也有很多不同，后者的关联是非常均匀的，或者说具有平移不变性，比如说一只狗的图像，在这里放是一只狗的图像，平移一下他还是那一只狗的图像。对于时空大数据就不一样了，比如北京的一条路和天津的一条路，甚至二环上的一条路和三环上的一条路表现可能是完全不一样的，这里面还有许多其他的因素在影响。比如说交通，影响交通的有天气，有PM2.5，有节假日，有其他特殊事件……这些都可能影响我们做预测。多种影响因素导致数据源可能也有很多，如何整合这些数据源也是一个很大的挑战。另外一个不同点是，时空大数据有可能噪音非常大。

因为有很多不同的影响因素，所以对一些问题，即使人也很难做出判断，这和我们看到的图像分类、机器翻译很不同，因为对这些问题我们至少人可以把它做得比较好，对于时空大数据的预测问题是人都很难进行判断的，反而是机器学习能够做得更好一点。

**应用深度学习进行时空大数据挖掘，第一个挑战就是如何抓住时空的关联性。**这里面涉及到几个问题：第一，你用什么样的学习框架，监督学习还是非监督学习，还有多任务学习、在线学习、增强学习……很多种方法，你用哪一种框架。这里并没有一个很成熟的做法，比如图像就用CNN；第二，你想用深度学习，什么样的网络结构比较好；第三，我刚才说了有很多的数据源，不同的数据源会产生不同的因子，我们如何构造一个网络，能够将这些不同质的因子综合起来，并且希望这个网络能够易于拓展，易于应用；第四，我们可能还要处理一些噪音很大，或者说数据缺失的问题。

## 深度学习在时空大数据的四个经典应用

- **网约车供需的预测**

**任务：一个区域在未来一段时间，有多少个网约车叫车订单将不会被满足？**

我们这个工作实际上是基于前一段时间滴滴公司的一个大数据竞赛。滴滴公司提供了一个月某个城市所有订单的资料，以及其他的一些比如说天气、PM2.5，还有一些路况的信息。具体的任务是，在他给定的几个区域内，对每个区域，你去预测未来15分钟有多少个订单将不会被满足，也就是说需求减去供给。

这个问题对于滴滴来说很重要，因为如果一个地方有很多订单没有被满足的话，他可能事先去调派一些出租车，使得供需达到平衡。另外对他们定价也有很重要的作用。这样的问题就是一个典型的时空大数据的预测问题。这个订单既有时间的属性，也有空间的属性，另外还有其他许多因素。这个问题并不简单，有很多数据源，有些数据源数据缺失也比较严重。

这里面我们发现有很多挑战，比如下面这张图中，这两个不同的位置的需求模式是非常不一样的。而且，有一些突发的事件，像需求突然增大，想抓住这种突然增大的趋势，也是有一定难度的。

![img](https://images.tmtpost.com/uploads/watermark/1400/a3510367df4c864270428413c7abedbc5d824f53_1400_691.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

传统的方法，比较常用的有线性回归、逻辑回归、支持向量级、gradient  boosting、随机森林等，这些方法在实际应用想要达到比较好的效果，一个很重要的工作就是要做很大量的特征工程，就是挖掘很多有预测能力的特征，有时甚至是很奇怪的特征，这些特征是可能对预测有帮助的因子。想达到一个非常有竞争力的效果，通常需要做很多很多这样不同的因子，而且有些因子实际上看起来并没有什么道理，或者讲不清楚有什么道理，但有时也有一定效果。一般来说参加这种机器学习比赛的队伍大多用gradient  boosting的方法，很多队伍都做了大量人工的因子，有的甚至都做了几百个因子。我们用深度学习的初衷，就是它有自己提取特征的能力，我们希望减小特征工程的人工的工作量，希望是只做一些基本的特征工程，也能达到一个非常有竞争力的预测的准确度。

同时我们也希望做一个比较易用的框架，使得我们比较容易的将不同类型的数据整合起来，比如说订单数据、日期数据、位置数据、天气等数据，都是不同质的，有些是数值型的，有些是种类，应该有一个方法，把它们的信息非常容易的都综合在一起。此外，每个数据源里面不同的数据有不同的预测能力，你得把里面的信息都抽取出来。再者，这些模块也希望比较灵活的，你可以往里添加模块，也就是添加数据源，有时有些信息可能是冗余信息，它并没有提供新的预测能力，即使添加了这个模块，也不会增加你的准确度，但是你也希望它不会影响降低准确度。

下面这张图，就是我们后面做的深度学习的一个大的网络框架的示意图。大家可以看到，我们有几个大的模块，每一个模块处理不同类型的数据。order part就是处理订单数据，Environment part处理环境信息，如路况、天气等，还有Identify  part就是处理时间，位置的ID就是我们要预测的时间和地点。我们最后是得到了一个端到端的一个模型，就是说我们能够直接将这些原始数据做简单的处理统计，然后就可以输入到这个网络里，然后这个网络就会从不同的数据源里抓取有预测能力的信息，最后给出比较准确的预测。

![img](https://images.tmtpost.com/uploads/watermark/1400/dd72eceeb434a5054fb531e5b027b3ceb337bfde_1400_794.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

我们这里面用了一个小技巧，就是在不同模块之间用了一个residual link，就是残差的连接，这个连接是由深度残差网络这个工作启发得到的，深度残差网络在钛坦白昨天的分享中秦涛博士也提到了（相关文章：http://www.tmtpost.com/2578654.html），是微软亚洲研究院做的一个很有名的工作，他们用这种残差的连接可以训练几百甚至上千层深度学习的网络。我们这里面的残差链接是从他们的工作受启发，但是我们的目的有所不同，一是为了方便优化，这是跟他们一样的。但另外一点，我们是希望能够使这些模块更容易的进行添加和删除。比如说你现在添加一个模块你直接加进来，比如里面有一个模块没有什么有预测能力的信息，实际上等价于一个权重为零的层，相当于信息直接从这个模块跳过去了，也不影响你的预测能力。

我们每个模块有不同的设计方法，下面这张图是我们订单数据的一个大致的结构。我们希望对比较典型的几个不同数据类型，设计比较通用和易用的几个网络结构来处理这些数据。

![img](https://images.tmtpost.com/uploads/watermark/1400/29e3ed102d33566e0a1f01a91cd8dd93c3068c12_1400_841.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

另外，在实际案例中，周一的订单和周二的订单、周日的订单的模式可能是完全不同的。一种方法是把周一的数据拿过来训练一个模型，对周二数据拿过来训练一个模型，我们的做法是，把他们都加在一起进行训练，但是给他们每一天有一个不同的权重，这个网络会自动的把这些权重学出来。

![img](https://images.tmtpost.com/uploads/watermark/1400/707f8fd8118b7d8f644dd6e900a65c114ea6395e_1400_706.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

下面这张图就是我们神经网络的预测效果，跟其他传统方法进行了比较，我们用的一样的特征。我们网络的一个前期的版本参加了滴滴的预测大赛，一共有1600多个队伍参加，我们获得了第二名。而且据我们所知，在前十名队伍里面只有我们一个队伍是用的深度学习的方法。所以说在时空大数据预测方面，深度学习还是有很潜力的。在比赛以后，我们又把这个网络的模块化、易用性、准确度做了进一步的提升。

![img](https://images.tmtpost.com/uploads/watermark/1400/88c3f0b0dc013b3bb8332dc7c2a6d82c9eed74bf_1400_843.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

下面这张图显示的是供给减去需求的曲线，大家可以看到，有很多突然出现的波峰，表示一个突发的打车高峰，比如说一个突发的事件来了，突然这个地方有很多打车的人，结果没有打到，就会有一个峰值。虚线是真实的值，蓝线是我们的预测，红线是大家常用的gradient boosting的方法预测的值。大家可以看到，神经网络抓这种峰值的能力还是比较强的。

![img](https://images.tmtpost.com/uploads/watermark/1400/06def22fdc348b99b208af7093b2152bb0b6c7ed_1400_628.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

同时我们认为，这个技术还可以应用到其他的供需、流量预测的这类问题上。杉数科技目前正在帮助国内某大型物流公司解决供应链问题，流量预测也是其中的一个很重要的环节，我作为杉数的合作科学家也在一起帮助解决这样的一个问题。深度学习是我们认为解决这个问题的一个非常好的解决方案。

- **出行时间的预测**

**任务：对给定的路径和出发时间，预估到达终点所用的时间**

这个任务具体是，你给定一个点作为起点，给定另外一个点作为终点，给出你起始的时间，我们要预测你走这条路需要花多少时间。实际上目前比如说百度地图、滴滴这些应用上都有这样的功能。现在出行时间的预测是根据当时的路况，但是假如我们从清华到朝阳的某个地方，路线比较长，可能当时某一段路是比较堵的，但我们开到那边其实已经不堵了，实际上我们实际的出行时间就比当时的预测要短。所以我们考虑用机器学习的方法来进行预测，而不是只用当时的路况进行预测。

前段时间有个数据城堡大数据预测比赛。在这个比赛里面，数据和前面的供需预测有很多相似的地方，但有一点重要的不同，就是要考虑轨迹的数据。也就是训练数据包含很多历史上开车的轨迹，这些轨迹都是GPS点，这些点上有时间的属性和空间的属性。

下面这张图是我们的大致的网络结构。这个结构跟我们前面的有些相似，也是模块化的，就是每种不同类的数据我们有一个模块去处理它。这里一个重要的不同就是有一个处理轨迹数据的新的模块（在图右上角），我们应用的是两层的LSTM结构，还有一些其他的技巧，比如说刚才提到的残差网络连接的这样一些技巧来连接模块。

![img](https://images.tmtpost.com/uploads/watermark/1400/cda5f59693778814109ade7281bdb8d68aab0827_1400_794.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

下面这张图，就是我们深度神经网络的预测效果，最后我们这个比赛是拿到了第三名。虽然不是第一，但是我们认为达到了预期的效果。首先我们并没有花很多精力去做特征工程，这是跟其他队伍非常不一样的。另外我认为我们这种模块化设计是比较通用的。另外我们的准确率也很有竞争力，前几名的的准确率都是非常非常接近的。

![img](https://images.tmtpost.com/uploads/watermark/1400/00aa42dd16a6515b3166b20d88ef6c09754139ff_1400_821.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

- **商店选址**

**任务：预测在什么地方开店比较好**

这个项目我们没有用到深度学习，但这是一个应用时空大数据比较好的例子。这是我们跟百度的大数据研究院合作的一个项目，应用了百度移动端的数据还有百度糯米、百度地图的一些数据。我们主要的目的是商店选址。比如说你想开一个咖啡店，我们来帮助你选择在什么地方开店比较好。

**我们基于的思想，大致就是首先来预测这些不同地点的供给以及需求。**这个需求是可以通过比如百度的数据得到的，比如说你在某一个地方搜了一下星巴克，意味着很大可能这有一个需求。供给就是已经开的店，这些店形成了一种竞争，可能已经能够提供一些服务满足一些需求了。我们想找的就是供需差别最大的，哪些地方我有很大的需求，但是供给仍然不足。

我们的工作主要分四步：

- 通过百度的数据预测供给；
- 找出已经有的店，然后计算出一个竞争的模型，就是每个地方你会有多少相似的店跟你竞争，然后这些店已经能够吸收多少需求；
- 找出这种供给和需求差比较大的一些这些地点，然后进行聚类，找出聚类的中心；
- 实际的进行选址。

我们做完这个项目做了一些实地的调研。下面这张图，我们预测了一个地点适合开海底捞，其实在我们做这个工作前面两个月就开了一家店海底捞，距离我们预测的地点非常近：

![img](https://images.tmtpost.com/uploads/watermark/1400/8a69fcc4d67cffe3cc280c7e3b7d0625adc3f422_1400_712.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

下面这张图是星巴克的例子。实际新开的点距离我们预测的地方只差200米：

![img](https://images.tmtpost.com/uploads/watermark/1400/fdad85ac78b658467d12deb253bddc5577d7322c_1400_866.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

- **预测到访**

**任务：在一个地点，会进哪家店**

我们有用户移动端的很多GPS数据，也有一些到店登录Wifi的数据，还有百度糯米的其他类相关的数据，我们目的是给定他当前的地点，我们想去预测他实际上是去了哪个店。

这件事情你可能认为挺简单，已经知道了地点你还不知道去了哪个店，但是实际上并不是那么简单，比如说你去了大悦城，里面有很多家店。你只知道一个大致的GPS地址，这个该怎么预测？我们预测的依据实际上是这个用户的历史信息，这个用户可能历史上去过某些店，将来很可能也去一些相似的店，另外有些相关性，比如说某些用户和某些用户的轨迹非常相似，有相关性，这种信息也会帮助我们进行预测。

这个预测我们也是用的深度神经网络，我们应用了跟前面很相似的处理轨迹信息的模块，最后也达到了比较好的效果。下面这张图就是我们这个方法预测的准确率，三就代表我们预测三个里面有多少准确的。预测一个基本上可以做到50%左右的准确率，如果预测5个就比较高，就可以做到80%的准确率。

![img](https://images.tmtpost.com/uploads/watermark/1400/1ba06499c46b3c90d90154146b87047c01aa2aa9_1400_1064.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

## 时空大数据的其他一些研究方向

我下面简单谈一下其他相关的问题，和我们计划下一步做的一些东西，不一定是深度学习。其中一个就是在线学习和增强学习。我刚才说的都是用时空大数据来做分析和预测，在现实应用中，预测只是帮助我们来进行决策。现在一般的方法是将预测和决策进行分离。我们先进行预测，然后再进行决策。决策往往是一个优化问题。

但是**我们现在想利用在线学习和增强学习这些强大的工具，将预测和决策这两个东西结合起来**。另外在线学习能够得到这个预测和决策的关系。一般说来，我们当然预测的越准确越好。但实际上有时候不需要预测到多准确，或者只对问题的某个方面而不是全部有好的预测，就已经可以做很好的决策了。

在在线学习里面有一个很著名的问题叫**专家问题**，就是说假设一个游戏有T轮，有N个不同的专家，每个专家在每一轮都会给我们一些不同的建议，我们每一轮要决定听哪个专家的。但是在那个专家做建议的时候我们实际上不知道他这个建议好还是差的。我们只知道以前这个专家做的建议都是好还是不好。在线学习会告诉我们每轮去听哪个专家的建议。最后我们想达到一个什么效果呢？我们可以达到在整个T轮中，我们的表现可以和最好的那个专家差不多。

专家问题有一个变种，就是下面这张图片。这个图片上面的人物是一个非常著名的信息论学家，他叫Thomas Cover，前几年过世了。他有一篇很著名的文章叫《Universal Portfolios》，可能翻译成**普遍投资组合**。这个是个什么问题呢？实际上跟专家问题非常相关的，他实际上考虑的是一个大家更感兴趣的问题就是股票的投资组合，就是说我现在假设有N个股票，我怎么样来进行投资，在这个股票中分散我的投资。目标是使得我最后的效果跟最好的那个股票比都不差。

![img](https://images.tmtpost.com/uploads/watermark/1400/2aaf5162664c078564ee39e34dd2ae35fa0fe92c_1400_1653.jpg?imageMogr2/auto-orient/strip/interlace/1/quality/85&ext=.jpg)

Thomas  Cover用的是很巧妙的乘法权重调整的方法，然后在不同股票数据中间做投资组合。实际上，不同股票数据也可以被认为是一种时空大数据，就是我们每个股票是个时间序列，不同股票之间也有一个空间上的相关性，比如说相同板块的股票表现有一定的相似性，不同板块独立性就比较强，这个是空间关联性。Thomas Cover就为我们预测加决策的这个在线学习方法提供了一个很好的基础。在当时，Thomas  Cover因为这个算法非常兴奋，就根据他的算法去开了一个对冲基金，好像效果还是不错的。

在线学习还有一个很著名的问题叫**多臂老虎机问题**。假设我们有N个不同的老虎机，每个老虎机实际上是有一个奖励的概率分布。你可以把它想成一个随机变量，你每拉一次老虎机，相当于从随机变量中取一个样本。你是希望找到最好的老虎机，也就是平均奖励最高的老虎机，这个实际上就是一个学习和决策结合的这样一个过程。事先我们并不知道这些老虎机的概率分布，我们需要学习，但最后我们需要决策把最好的老虎机找出来。

多臂老虎机问题的算法告诉我们一件什么事情呢？就是说我们实际上并不需要都把所有的老虎机都学习得很好，我们不需要对所有老虎机的估计准确率都很高，我们只需要对比较好的老虎机的准确率比较高就可以，就可以做很好的决策了，而那些比较差的我们不需要学习到多好。那么这个多臂老虎机问题也是控制优化，在线学习里面一个很经典的问题，也有很多很多的应用，我认为也是我们做时空数据学习加决策的这样一个很好的框架。

**另外一个我认为比较有意义的时空大数据的研究方向就是生成模型**。比如说我们如何虚拟的生成订单数据，或者我们虚拟生成交通流量，或者虚拟市场交易数据。那么有了这个生成模型，我们才能说我们完全理解了这个模型。一方面生成模型能帮助我们理解，另一方面我们如果有了生成模型，我们就能够用它来模拟新的环境。另外能用它来生成新的数据，来进行训练，就相当于是增多了我们的训练数据，也可以用来测试新的场景。有很多传统的时间序列和空间计量经济的模型，比如说时间序列模型，大家可能知道线性的ARMA模型还有GARCH模型，比如还有一些随机过程，比如说空间点过程模型，但是这些模型都是相对简单，可能很难抓住一个实际复杂的有很多因素决定的过程，其中有很多非线性的特征和细节。我们可能需要利用深度学习生成模型去抓住这种复杂的过程。比如说用深度学习里的authencoder，比如说更新的对抗生成网络这样一些生成模型。